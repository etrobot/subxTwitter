<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GPT Subscripton to Twitter List</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="/static/style.css">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7398757278741889"
         crossorigin="anonymous"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XV4CMHELK9"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('config', 'G-XV4CMHELK9');
    </script>
</head>

<body class="bg-gray-50 dark">

<div class="sidebar w-full max-w-sm items-center">
    <div>
        <button id="theme-toggle" class="px-2 rounded">üåí</button>
    </div>
    <div class="w-full text-center font-bold">
        <div class="title">GPT Subscripton to Twitter List</div>
    </div>
    <div class="text-xs my-4 w-80 m-auto leading-loose">
        <a href="/lang/zh-CN" class="border-b-2 mr-2">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        <a href="/lang/zh-TW" class="border-b-2 mr-2">ÁπÅÈ´î‰∏≠Êñá</a>
        <a href="/lang/en" class="border-b-2 mr-2">English</a>
        <a href="/lang/ja" class="border-b-2 mr-2">Êó•Êú¨Ë™û</a>
        <a href="/lang/ko" class="border-b-2 mr-2">ÌïúÍµ≠Ïñ¥</a>
        <a href="/lang/es" class="border-b-2 mr-2">Espa√±ol</a>
        <a href="/lang/pt" class="border-b-2 mr-2">Portugu√™s</a>
        <a href="/lang/de" class="border-b-2 mr-2">Deutsch</a>
        <a href="/lang/fr" class="border-b-2 mr-2">Fran√ßais</a>
        <a href="/lang/ar" class="border-b-2 mr-2">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        <a href="/lang/id" class="border-b-2 mr-2">Bahasa Indonesia</a>
        <a href="/lang/ms" class="border-b-2 mr-2">Bahasa Melayu</a>
        <a href="/lang/tl" class="border-b-2 mr-2">Filipino</a>
        <a href="/lang/vi" class="border-b-2 mr-2">Ti·∫øng Vi·ªát</a>
        <a href="/lang/pl" class="border-b-2 mr-2">Polski</a>
        <a href="/lang/nl" class="border-b-2 mr-2">Nederlands</a>
        <a href="/lang/th" class="border-b-2 mr-2">‡πÑ‡∏ó‡∏¢</a>
        <a href="/static/privacy.html" class="text-blue-500 border-b-2 mr-2">Privacy</a>
    </div>
<div class="w-full text-center">
    <a target=_blank href="https://discord.com/api/oauth2/authorize?client_id=1185508364701151284&permissions=8797166831616&scope=bot" class="mt-4 text-white bg-blue-700 hover:bg-blue-800 focus:ring-4 focus:ring-blue-300 font-medium rounded-full text-sm px-5 py-1 dark:bg-blue-600 dark:hover:bg-blue-700 focus:outline-none dark:focus:ring-blue-800">Chat GPT-4 Free</a>
</div>
</div>
<div id="parentContainer" class="flex flex-wrap m-2">
    
    <div class="flex flex-col rounded-xl my-1 p-4 bg-gray-500 bg-opacity-5 max-w-screen-md mx-auto">
        <div class="flex w-full items-center space-x-2 max-w-screen-lg">
            <div class="w-16 h-10 rounded overflow-hidden">
                <img src="https://pbs.twimg.com/list_banner_img/1733654690490200064/ig-cUZfi?format=jpg&name=360x360" class="object-none w-full h-full"/>
            </div>

            <a class="text-sm w-full" href="https://twitter.com/i/lists/1733652180576686386">
                <div class="font-bold">AGI Thoughts</div>
                <div class="text-gray-500">1733652180576686386</div>
            </a>
            <button class="subx bg-blue-400 text-white px-3 py-1  rounded-full" value="1733652180576686386">üîî</button>
        </div>
        <div class="mt-2 text-sm">
            <h2>Les Tweets</h2>
<p><a style="color:#5da2ff;" href="https://x.com/ylecun/status/1748557610855399710#m">20 Jan 2024 04:06:35 @ylecun</a>: AI could help with carbon capture.</p>
<p>Quoting: Meta and <a href="https://x.com/GeorgiaTech" title="Georgia Tech">@GeorgiaTech</a> researchers released a dataset + AI models to help accelerate research on Direct Air Capture ‚Äî a key technology needed to combat climate change.  </p>
<p>Get the models &amp; dataset ‚û°Ô∏è <a href="https://bit.ly/47gEKfy">bit.ly/47gEKfy</a>  </p>
<p>OpenDAC23 is the largest ever dataset of it's kind at the DFT level of accuracy and we hope that this work will help to accelerate research across this important field of study.</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGEJk_cpbkAAksjn.jpg" />
x.com/AIatMeta/status/1748070705676386347#m</p>
<p>Meta et les chercheurs de <a href="https://x.com/GeorgiaTech" title="Georgia Tech">@GeorgiaTech</a> ont publi√© un ensemble de donn√©es et des mod√®les d'IA pour acc√©l√©rer la recherche sur la capture directe de l'air, une technologie cl√© n√©cessaire pour lutter contre le changement climatique.</p>
<p>Obtenez les mod√®les et l'ensemble de donn√©es ‚û°Ô∏è <a href="https://bit.ly/47gEKfy">bit.ly/47gEKfy</a></p>
<p>OpenDAC23 est le plus grand ensemble de donn√©es de ce type au niveau de pr√©cision DFT et nous esp√©rons que ce travail contribuera √† acc√©l√©rer la recherche dans ce domaine important.</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGEJk_cpbkAAksjn.jpg" />
x.com/AIatMeta/status/1748070705676386347#m</p>
<hr>

<p><a style="color:#5da2ff;" href="https://x.com/IlyesBatatia/status/1742238047725326431#m">02 Jan 2024 17:34:54 @IlyesBatatia</a>: Our large collaborative effort shows the impressive range of applicability of foundational ML potentials, developed using only open-source data and software. More than 30 applications, including MOFs, catalysis, water, and more simulated with one model.<br />
<a href="https://arxiv.org/abs/2401.00096">arxiv.org/abs/2401.00096</a></p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGC2nHHRW0AAKijC.png" /></p>
<p>Notre vaste effort collaboratif montre l'impressionnante gamme d'applicabilit√© des potentiels fondamentaux de l'apprentissage automatique, d√©velopp√©s uniquement √† l'aide de donn√©es et de logiciels open-source. Plus de 30 applications, dont des MOF, la catalyse, l'eau, et bien d'autres, ont √©t√© simul√©es avec un seul mod√®le.  </p>
<p><a style="color:#5da2ff;" href="https://arxiv.org/abs/2401.00096">arxiv.org/abs/2401.00096</a></p>
<hr>

<p><a style="color:#5da2ff;" href="https://x.com/AravSrinivas/status/1748533903038521731#m">20 Jan 2024 02:32:23 @AravSrinivas</a>: Paris is accelerating</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGEQKUNebgAAPCOJ.jpg" /></p>
<p>Paris s'acc√©l√®re</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGEQKUNebgAAPCOJ.jpg" /></p>
<hr>

<p><a style="color:#5da2ff;" href="https://x.com/NYUDataScience/status/1748391439187472563#m">19 Jan 2024 17:06:17 @NYUDataScience</a>: La science des donn√©es est vraiment pour tout le monde ! Nous sommes ravis d'annoncer la sortie du nouveau livre de cours et de la s√©rie vid√©o inspir√©e par "Data Science for Everyone", notre cours de premier cycle populaire.</p>
<p>La science des donn√©es est vraiment pour tout le monde ! Nous sommes ravis d'annoncer la sortie du nouveau livre de cours et de la s√©rie vid√©o inspir√©e par "Data Science for Everyone", notre cours de premier cycle populaire.</p>
<hr>

<p><a style="color:#5da2ff;" href="https://x.com/ylecun/status/1748531221989142822#m">20 Jan 2024 02:21:44 @ylecun</a>: Supprimer un espace peut faire perdre tout son sens √† une phrase, par exemple "Open AI" devient "OpenAI" et aussi "Le Cun" devient "LeCun" üò±</p>
<p>Quoting: Bindu Reddy (@bindureddy): "Zuck et Yann LeCunn resteront dans l'histoire de l'humanit√© ! Se battre pour Open AI lorsque les titulaires ont cherch√© √† le fermer ! Incroyable √† quel point les vibrations de Meta ont chang√© au cours de la derni√®re ann√©e. Je pense qu'il est peut-√™tre temps de changer de nom - Meta en OpenAI üòâ" | Nitter
x.com/bindureddy/status/1748458990974247401#m</p>
<p>Zuck et Yann LeCunn resteront dans l'histoire de l'humanit√© ! Se battre pour Open AI lorsque les titulaires ont cherch√© √† le fermer ! Incroyable √† quel point les vibrations de Meta ont chang√© au cours de la derni√®re ann√©e. Je pense qu'il est peut-√™tre temps de changer de nom - Meta en OpenAI üòâ</p>
<hr>

<p><a style="color:#5da2ff;" href="https://x.com/ylecun/status/1748526916104020125#m">20 Jan 2024 02:04:37 @ylecun</a>: J'ai un nom plus simple pour "group attention": pooling.</p>
<p>Quoting: Cameron R. Wolfe, Ph.D. (@cwolferesearch): "Les impressionnantes capacit√©s d'apprentissage en contexte des LLM ont cr√©√© le besoin de fen√™tres contextuelles plus grandes. R√©cemment, les chercheurs ont d√©couvert que nous pouvons facilement √©tendre la fen√™tre contextuelle d'un LLM pr√©-entra√Æn√© avec un simple tour de passe-passe (et sans formation suppl√©mentaire)‚Ä¶</p>
<p>Quelle est la fen√™tre contextuelle ? Lors du pr√©-entra√Ænement, un LLM voit des s√©quences d'entr√©e d'une certaine longueur. Ce choix de longueur de s√©quence lors du pr√©-entra√Ænement devient la longueur de contexte du mod√®le, ou la s√©quence de texte de longueur maximale que le mod√®le peut traiter. Au-del√† de cette longueur de contexte, le mod√®le peut se comporter de mani√®re impr√©visible et produire des r√©sultats incorrects.</p>
<p>Pourquoi avons-nous besoin d'une grande fen√™tre contextuelle ? Les praticiens veulent la capacit√© de faire passer plus de donn√©es dans la fen√™tre contextuelle du LLM pour permettre des applications plus complexes via des approches telles que l'apprentissage √† quelques exemples (ou m√™me des approches de formulation plus complexes comme la formulation de cha√Ænes de pens√©e) et la g√©n√©ration augment√©e par r√©cup√©ration (RAG). Bien que plusieurs LLM √† long contexte aient √©t√© publi√©s (par ex., Claude 2.1 et GPT-4-Turbo), tous les LLM n'ont pas √©t√© form√©s pour prendre en charge un contexte long, et les LLM open-source tendent √† ne prendre en charge que des contextes plus courts par rapport √† leurs alternatives propri√©taires.</p>
<p>Extension de la fen√™tre contextuelle. Pour √©tendre la fen√™tre contextuelle d'un LLM pr√©-entrain√©, nous pourrions affiner le mod√®le sur des exemples de s√©quences plus longues, mais une telle approche pourrait amener le mod√®le √† surajuster √† des exemples sp√©cifiques de s√©quences longues. Plusieurs approches ont √©t√© propos√©es pour √©tendre la fen√™tre contextuelle d'un LLM sans (ou avec un minimum de) affinage, y compris PI, CLEX, et YARN. De plus, des approches couramment utilis√©es telles que ALiBi et RoPE permettent aux LLM de traiter des entr√©es plus longues lors de l'inf√©rence que celles vues lors de l'entra√Ænement.</p>
<p>Pourquoi les LLM ne peuvent-ils pas g√©n√©raliser √† des s√©quences plus longues ? Le probl√®me cl√© auquel les LLM sont confront√©s en g√©n√©ralisant √† des fen√™tres contextuelles plus longues est li√© aux encodages positionnels hors distribution, o√π le LLM est expos√© √† des distances relatives et des positions de jetons qui d√©passent ce qui a √©t√© vu lors de l'entra√Ænement. Nous pouvons facilement r√©soudre ce probl√®me en remappant simplement les positions invisibles sur des positions qui ont √©t√© rencontr√©es lors de l'entra√Ænement.</p>
<p>‚ÄúPour r√©soudre cela, une solution intuitive et pratique serait de remapper les positions relatives invisibles sur celles rencontr√©es lors du pr√©-entra√Ænement, √©tendant ainsi la capacit√© des LLM √† traiter naturellement des contextes plus longs.‚Äù - extrait de l'article Self Extend Quoted tweet</p>
<p>Grouped Attention. Dans "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", les auteurs avancent que les LLM ont une capacit√© inh√©rente √† traiter de longues s√©quences qui peut √™tre exploit√©e sans formation suppl√©mentaire. Nous pouvons utiliser une op√©ration de plancher qui effectue une division enti√®re sur les indices de position de telle sorte que l'indice de position maximum vu lors de l'inf√©rence ne d√©passe pas la longueur de contexte pr√©d√©finie du mod√®le. Bien que cela puisse amener des jetons adjacents √† √™tre attribu√©s au m√™me indice de position, cela fonctionne n√©anmoins bien en pratique car :</p>
<ol>
<li>La position pr√©cise du jeton est moins importante que l'ordre relatif lorsqu'on essaie de comprendre une s√©quence de texte.</li>
<li>Les courtes s√©quences de jetons ont tendance √† n'avoir qu'un seul ordre valide, donc les attribuer au m√™me indice de position a peu d'impact pratique.</li>
</ol>
<p>Une telle approche, appel√©e "attention group√©e" car nous regroupons plusieurs jetons pour l'incorporation de position, fonctionne de fa√ßon comparable aux techniques d'affinage pour √©tendre la fen√™tre contextuelle et ne n√©cessite que des modifications de code minimales (c'est-√†-dire seulement quatre lignes suppl√©mentaires en PyTorch).</p>
<p>Self Extend. Si nous appliquons na√Øvement l'attention group√©e, les performances de mod√©lisation du langage se d√©gradent l√©g√®rement, car les jetons dans toute la s√©quence sont cartographi√©s dans des groupes partageant le m√™me indice de position. Pour r√©soudre ce probl√®me, nous devons r√©aliser que les jetons voisins sont les plus importants lors de la g√©n√©ration d'un jeton avec un LLM. Nous pouvons √©liminer cette d√©gradation des performances en :</p>
<ol>
<li>D√©finissant une taille de voisinage des jetons les plus r√©cents sur lesquels une attention normale est appliqu√©e.</li>
<li>Utilisation de l'attention group√©e pour les jetons qui sont plus √©loign√©s dans la s√©quence.</li>
</ol>
<p>Cette astuce finale forme la technique Self Extend, qui peut √™tre utilis√©e pour augmenter la longueur de contexte de n'importe quel LLM au moment de l'inf√©rence sans avoir besoin d'affiner le mod√®le.</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="Image" src="https://nitter.catsarch.com/pic/media%2FGEOJzhKXMAA5_hB.jpg" />
x.com/cwolferesearch/status/1748393116338409890#m</p>
        </div><br><br><br><br><br><br>
    </div>

</div>
<div class="flex flex-col fixed bottom-0 w-full subcard backdrop-filter backdrop-blur-lg">
    <div class="text-sm px-2 pb-2 pt-2">
        <form id="subscribe-form" action="/subscribe" method="post" class="flex flex-col mb-2">
            <div class="flex items-center  w-full">
                <input type="number" name="target_id" placeholder="Enter target ID" required
                       class="m-1 border border-gray-300 rounded px-4 py-1 focus:outline-none focus:ring focus:border-blue-300  w-full">
                <a href="https://business.twitter.com/en/blog/twitter-101-lists.html" target="_blank"
                   rel="noopener noreferrer"
                   class="border border-white rounded px-3 py-1  mr-2">
                    ?
                </a>
            </div>
            <div class="flex items-center mt-1 w-full">
                <input type="email" name="email" placeholder="Enter your email address" required
                       class="m-1 border border-gray-300 rounded px-4 py-1 focus:outline-none focus:ring focus:border-blue-300 w-full max-w-xs">
                <select id="language-select"
                        class="w-1/2 border border-white mr-1 ml-auto bg-transparent py-1 rounded">
                    <option value="zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</option>
                    <option value="zh-TW">ÁπÅÈ´î‰∏≠Êñá</option>
                    <option value="en">English</option>
                    <option value="ja">Êó•Êú¨Ë™û</option>
                    <option value="ko">ÌïúÍµ≠Ïñ¥</option>
                    <option value="es">Espa√±ol</option>
                    <option value="pt">Portugu√™s</option>
                    <option value="de">Deutsch</option>
                    <option value="fr">Fran√ßais</option>
                    <option value="ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                    <option value="id">Bahasa Indonesia</option>
                    <option value="ms">Bahasa Melayu</option>
                    <option value="tl">Filipino</option>
                    <option value="vi">Ti·∫øng Vi·ªát</option>
                    <option value="pl">Polski</option>
                    <option value="nl">Nederlands</option>
                    <option value="th">‡πÑ‡∏ó‡∏¢</option>
                </select>
            </div>
            <div class="flex mt-1 w-full">
                <input type="time" id="mail-time" required
                       class="w-20 m-1 border border-gray-300 rounded-full py-1 focus:ring focus:border-blue-300">
                <button type="submit" class="bg-blue-500 text-white py-1 rounded-full m-1 w-full"
                        id="subscribe-btn">Subscribe
                </button>
                <input type="hidden" name="mail_time" id="mail-time-timestamp" value="">
            </div>
            <input type="hidden" name="current_language" id="current-language" value="">

        </form>
        <span class="text-xs" id="time-label">Daily Push Time</span><span class="fixed bottom-2 right-2">¬© subxTwitter 2024</span>
    </div>
</div>
</body>
<script src="/static/index.js"></script>
</html>