<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GPT Subscripton to Twitter List</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="/static/style.css">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7398757278741889"
         crossorigin="anonymous"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XV4CMHELK9"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('config', 'G-XV4CMHELK9');
    </script>
</head>

<body class="bg-gray-50 dark">

<div class="sidebar w-full max-w-sm items-center">
    <div>
        <button id="theme-toggle" class="px-2 rounded">🌒</button>
    </div>
    <div class="w-full text-center font-bold">
        <div class="title">GPT Subscripton to Twitter List</div>
    </div>
    <div class="text-xs my-4 w-80 m-auto leading-loose">
        <a href="/lang/zh-CN" class="border-b-2 mr-2">简体中文</a>
        <a href="/lang/zh-TW" class="border-b-2 mr-2">繁體中文</a>
        <a href="/lang/en" class="border-b-2 mr-2">English</a>
        <a href="/lang/ja" class="border-b-2 mr-2">日本語</a>
        <a href="/lang/ko" class="border-b-2 mr-2">한국어</a>
        <a href="/lang/es" class="border-b-2 mr-2">Español</a>
        <a href="/lang/pt" class="border-b-2 mr-2">Português</a>
        <a href="/lang/de" class="border-b-2 mr-2">Deutsch</a>
        <a href="/lang/fr" class="border-b-2 mr-2">Français</a>
        <a href="/lang/ar" class="border-b-2 mr-2">العربية</a>
        <a href="/lang/id" class="border-b-2 mr-2">Bahasa Indonesia</a>
        <a href="/lang/ms" class="border-b-2 mr-2">Bahasa Melayu</a>
        <a href="/lang/tl" class="border-b-2 mr-2">Filipino</a>
        <a href="/lang/vi" class="border-b-2 mr-2">Tiếng Việt</a>
        <a href="/lang/pl" class="border-b-2 mr-2">Polski</a>
        <a href="/lang/nl" class="border-b-2 mr-2">Nederlands</a>
        <a href="/lang/th" class="border-b-2 mr-2">ไทย</a>
        <a href="/static/privacy.html" class="text-blue-500 border-b-2 mr-2">Privacy</a>
    </div>
<div class="w-full text-center">
    <a target=_blank href="https://discord.com/api/oauth2/authorize?client_id=1185508364701151284&permissions=8797166831616&scope=bot" class="mt-4 text-white bg-blue-700 hover:bg-blue-800 focus:ring-4 focus:ring-blue-300 font-medium rounded-full text-sm px-5 py-1 dark:bg-blue-600 dark:hover:bg-blue-700 focus:outline-none dark:focus:ring-blue-800">Chat GPT-4 Free</a>
</div>
</div>
<div id="parentContainer" class="flex flex-wrap m-2">
    
    <div class="flex flex-col rounded-xl my-1 p-4 bg-gray-500 bg-opacity-5 max-w-screen-md mx-auto">
        <div class="flex w-full items-center space-x-2 max-w-screen-lg">
            <div class="w-16 h-10 rounded overflow-hidden">
                <img src="https://pbs.twimg.com/list_banner_img/1733654690490200064/ig-cUZfi?format=jpg&name=360x360" class="object-none w-full h-full"/>
            </div>

            <a class="text-sm w-full" href="https://twitter.com/i/lists/1733652180576686386">
                <div class="font-bold">AGI Thoughts</div>
                <div class="text-gray-500">1733652180576686386</div>
            </a>
            <button class="subx bg-blue-400 text-white px-3 py-1  rounded-full" value="1733652180576686386">🔔</button>
        </div>
        <div class="mt-2 text-sm">
            <p>Certainly! Here is the compilation of the above tweets in Filipino with the required  format:</p>
<hr />
<h4>20 Ene 2024 04:13:18 @ylecun</h4>
<p>https://x.com/ylecun/status/1748559301923274970#m</p>
<p>Self-Rewarding LLMs.
From FAIR+NYU.</p>
<hr />
<p>Quoting:</p>
<blockquote>
<p>Jason Weston (@jaseweston): "🚨Bagong papel!🚨
Self-Rewarding LMs
- Ang LM mismo ay nagbibigay ng sarili nitong mga gantimpala sa pamamagitan ng LLM-bilang-hukom sa panahon ng Iteratibong DPO
- Ang kakayahan ng pag-mowdelo ng gantimpala ay nagpapabuti sa panahon ng pagsasanay kaysa manatiling pare-pareho
...nagbubukas ito ng pinto sa higit sa makataong puna?
<a href="https://arxiv.org/abs/2401.10020">arxiv.org/abs/2401.10020</a>
🧵(1/5)" | Nitter
x.com/jaseweston/status/1748158323369611577#m</p>
</blockquote>
<p>🚨Bagong papel!🚨
Self-Rewarding LMs
- Ang LM mismo ay nagbibigay ng sarili nitong mga gantimpala sa pamamagitan ng LLM-bilang-hukom sa panahon ng Iteratibong DPO
- Ang kakayahan ng pag-mowdelo ng gantimpala ay nagpapabuti sa panahon ng pagsasanay kaysa manatiling pare-pareho
...nagbubukas ito ng pinto sa higit sa makataong puna?
<a href="https://arxiv.org/abs/2401.10020">arxiv.org/abs/2401.10020</a>
🧵(1/5)</p>
<hr />
<h4>20 Ene 2024 04:06:35 @ylecun</h4>
<p>https://x.com/ylecun/status/1748557610855399710#m</p>
<p>Ang AI ay maaaring makatulong sa panghuli ng karbon.</p>
<hr />
<p>Quoting:</p>
<blockquote>
<p>Meta at <a href="https://x.com/GeorgiaTech" title="Georgia Tech">@GeorgiaTech</a> researchers released a dataset + AI models to help accelerate research on Direct Air Capture — isang pangunahing teknolohiyang kailangan upang labanan ang pagbabago ng klima.</p>
<p>Kunin ang mga modelo &amp; dataset ➡️ <a href="https://bit.ly/47gEKfy">bit.ly/47gEKfy</a></p>
<p>Ang OpenDAC23 ang pinakamalaking dataset ng ganitong uri sa antas ng DFT at inaasahan naming ang gawang ito ay makakatulong upang mapabilis ang pananaliksik sa larangang ito ng malaking kahalagahan.</p>
<p><img style="max-height: 16rem;margin:1rem;" alt="" src="https://nitter.catsarch.com/pic/media%2FGEJk_cpbkAAksjn.jpg" />
x.com/AIatMeta/status/1748070705676386347#m</p>
</blockquote>
<p>Ang Meta at mga mananaliksik mula sa <a href="https://x.com/GeorgiaTech" title="Georgia Tech">@GeorgiaTech</a> ay naglabas ng isang dataset + mga modelo ng AI upang mapabilis ang pananaliksik sa Direct Air Capture — isang pangunahing teknolohiyang kailangan upang labanan ang pagbabago ng klima.</p>
<p>Kunin ang mga modelo &amp; dataset ➡️ <a href="https://bit.ly/47gEKfy">bit.ly/47gEKfy</a></p>
<p>Ang OpenDAC23 ay ang pinakamalaking dataset ng ganitong uri sa antas ng DFT at umaasa kami na ang gawang ito ay makakatulong upang mapabilis ang pananaliksik sa larangang ito ng malaking kahalagahan.</p>
<hr />
<h4>02 Ene 2024 17:34:54 @IlyesBatatia</h4>
<p>https://x.com/IlyesBatatia/status/1742238047725326431#m</p>
<p>Ang aming malaking pagtutulungan ay nagpapakita ng kahanga-hangang saklaw ng aplikasyon ng mga mahalagang potensyal ng ML, na binuo gamit lamang ang mga bukas na pinagmumulan ng data at software. Higit sa 30 aplikasyon, kabilang ang MOFs, katalisis, tubig, at iba pa na sinimulang simulan gamit ang isang modelo.
<a href="https://arxiv.org/abs/2401.00096">arxiv.org/abs/2401.00096</a></p>
<hr />
<h4>20 Ene 2024 02:32:23 @AravSrinivas</h4>
<p>https://x.com/AravSrinivas/status/1748533903038521731#m</p>
<p>Ang Paris ay bumibilis</p>
<hr />
<h4>19 Ene 2024 17:06:17 @NYUDataScience</h4>
<p>https://x.com/NYUDataScience/status/1748391439187472563#m</p>
<p>Ang agham ng data ay tunay na para sa lahat! Excited kami na ipahayag ang paglabas ng bagong course book at video series na pinagmulan ng "Data Science for Everyone", ang aming sikat na kursong pang-undergraduate.</p>
<hr />
<h4>20 Ene 2024 02:21:44 @ylecun</h4>
<p>https://x.com/ylecun/status/1748531221989142822#m</p>
<p>Ang pagsasama ng mga letratang may puwang ay maaaring magpabago ng kahulugan ng isang parirala, tulad ng
"Open AI" -&gt; "OpenAI"
at bilang
"Le Cun" -&gt; "LeCun" 😱</p>
<hr />
<p>Quoting:</p>
<blockquote>
<p>Bindu Reddy (@bindureddy): "Zuck at Yann LeCunn ay magiging mga bayani sa kasaysayan ng tao!</p>
<p>Lumalaban para sa Open AI nang ang mga nakaupo ay nagsikap na isara ito!</p>
<p>Hindi maaaring paniwalaan kung gaano kabilis nagbago ang vibe mula sa Meta sa nakaraang taon. 🤯🤯</p>
<p>Iyan at marahil panahon na para sa pagbabago ng pangalan - Meta papunta sa OpenAI 😉" | Nitter
x.com/bindureddy/status/1748458990974247401#m</p>
</blockquote>
<p>Si Zuck at Yann LeCunn ay magiging mga bayani sa kasaysayan ng tao!</p>
<p>Lumalaban para sa Open AI nang ang mga nakaupo ay nagsikap na isara ito!</p>
<p>Hindi maaaring paniwalaan kung gaano kabilis nagbago ang vibe mula sa Meta sa nakaraang taon. 🤯🤯</p>
<p>Iyan at marahil panahon na para sa pagbabago ng pangalan - Meta papunta sa OpenAI 😉</p>
<hr />
<h4>20 Ene 2024 02:04:37 @ylecun</h4>
<p>https://x.com/ylecun/status/1748526916104020125#m</p>
<p>Mayroon akong isang mas simple na pangalan para sa "group attention": pooling.</p>
<hr />
<p>Quoting:</p>
<blockquote>
<p>Cameron R. Wolfe, Ph.D. (@cwolferesearch): "Ang kahanga-hangang kakayahan ng LLMs sa pag-aaral ng konteksto ay lumikha ng pangangailangan para sa mas malawak na mga window ng konteksto. Kamakailan lamang, natuklasan ng mga mananaliksik na nai-extend ng madali ang window ng konteksto ng isang pre-trained na LLM sa pamamagitan ng isang simpleng trick (at walang extra na pagsasanay)...</p>
<p>Ano ang window ng konteksto? Sa panahon ng pretraining, nakakakita ng mga input sequence na may partikular na haba ang LLM. Ang piling ng haba ng sequence na ito sa panahon ng pretraining ay naging kontekstong haba ng modelo, o ang maximum-length sequence ng teksto na kaya ng modelo na prosesuhin. Lampas dito, maaaring hindi inaasahang umakto ang modelo at mag-produce ng maling output.</p>
<p>Bakit kailangan natin ng malaking window ng konteksto? Gusto ng mga praktisyoner na magkaroon ng kakayahan na ipasa ang mas maraming data sa window ng konteksto ng LLM upang mapagkakataon ang mas komplikadong aplikasyon sa pamamagitan ng mga approach tulad ng few-shot learning (o maging mas komplikadong approach tulad ng chain of thought prompting) at retrieval augmented generation (RAG). Bagamat may ilan nang long-context LLMs na nailabas (tulad ng Claude 2.1 at GPT-4-Turbo), hindi lahat ng LLM ay naisanay upang suportahan ang mahabang konteksto, at ang mga open-source LLM ay may tendensiyang suportahan lamang ang mas maikling konteksto kumpara sa kanilang mga proprietary na kapalit.</p>
<p>Pahabaan ng window ng konteksto. Upang palawakin  ang window ng konteksto ng isang pre-trained na LLM, maaari nating i-finetune ang modelo sa mga halimbawa ng mas mahabang sequences, ngunit ang ganitong approach ay maaaring magdulot ng overfitting sa partikular na halimbawa ng mahabang sequences. Maraming paraan na ipinanukala upang palawakin ang window ng konteksto ng isang LLM ng walang (o minimal) na finetuning, kabilang ang PI, CLEX, at YARN. Bukod pa rito, ang mga kadalasang ginagamit na paraan tulad ng ALiBi at RoPE ay nagpapahintulot sa mga LLM na maka-handle ng mas mahabang inputs sa pagdadala kumpara sa kanilang natutunan sa pagsasanay.</p>
<p>Bakit hindi makahanda ang LLMs sa mas mahahabang sequences? Ang pangunahing isyu na hinaharap ng LLMs sa pangkalahatan ay may kaugnayan sa out-of-distribution positional encodings, kung saan ang LLM ay inilalantad sa relative distances at token positions na lampas sa inilantad sa panahon ng pagsasanay. Madaling ma-address ang isyung ito sa pamamagitan ng simpleng pag-remap ng mga hindi nakikitang positions sa mga positions na natagpuan sa panahon ng pagsasanay.</p>
<p>"Upang ma-address ito, ang isang matalinong at praktikal na solusyon ay ang i-remap ang hindi nakikitang relative positions patungo sa mga na-encounter sa panahon ng pretraining, na sa gayon ay pinalalawak ang kakayahan ng LLMs na maghandle ng mas mahahabang konteksto nang natural." - mula sa Self Extend papel</p>
<p>Grouped Attention. Sa "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", ipinapahayag ng mga awtor na mayroon nang inherenteng kakayahan ang LLMs sa pag-handle ng mahabang sequences na maaaring gamitin ng walang karagdagang pagsasanay. Maaari nating gamitin ang isang FLOOR operation na nagpapakita ng integer division sa mga position indices sa gayong paraang ang maximum position index na nakikita sa panahon ng inference ay hindi lalampas sa tinukoy na kontekstong haba ng modelo. Bagamat maaaring magreresulta ito sa pagkakagroup ng mga kalapit na token sa parehong position index, maayos pa rin ito sa praktika dahil:</p>
<ol>
<li>Ang eksaktong posisyon ng token ay hindi gaanong mahalaga kumpara sa relative ordering kapag sinusubukang intindihin ang isang sequence ng teksto.</li>
<li>Ang maikli at magkakatabing sequences ng mga token tend to have one valid ordering lamang, kaya't ang pag-a-assign sa kanila sa parehong position index ay may kaunting epekto sa praktika.</li>
</ol>
<p>Ang approach na ito, tinatawag na "grouped attention" dahil nilalagyan natin ng grupo ang ilang mga token sa parehong position embedding, ay kasing galing sa mga finetuning techniques para palawakin ang window ng konteksto at kailangan lamang ng minimal na pag-babago sa code (hal. apat na karagdagang linya sa PyTorch).</p>
<p>Self Extend. Kung ating pinag-apply ng walang malay ang grouped attention, ang performance ng language modeling ay bahagyang nag-deteriorate, dahil ang mga token sa buong sequences ay namap sa grupos na may parehong position index. Upang malutas ang problema na ito, kailangan nating ma-realize na ang mga kalapit na token ang pinakamahalaga sa pagge-generate ng isang token sa isang LLM. Kaya, maaari nating masugpo ang pag-deteriorate sa performance sa pamamagitan ng:</p>
<ol>
<li>Pagtukoy sa isang neighborbood size ng pinakabagong mga token kung saan ang normal na attention ay ipinapakita.</li>
<li>Paggamit ng grouped attention para sa mga token na malayong nasa loob ng sequence.</li>
</ol>
<p>Ang isang simpleng trick na ito ay sumasalamin sa Self Extend technique, na maaaring gamitin upang palawakin ang kontekstong haba ng anumang LLM sa panahon ng inference nang walang pangangailangan para sa finetuning." | Nitter
x.com/cwolferesearch/status/1748393116338409890#m</p>
</blockquote>
<p>Ang kahanga-hangang kakayahan ng LLMs sa pag-aaral ng konteksto ay lumikha ng pangangailangan para sa mas malawak na mga window ng konteksto. Kamakailan lamang, natuklasan ng mga mananaliksik na nai-extend ng madali ang window ng konteksto ng isang pre-trained na LLM sa pamamagitan ng isang simpleng trick (at walang extra na pagsasanay)…</p>
<p>Ano ang window ng konteksto? Sa panahon ng pretraining, nakakakita ng mga input sequence na may partikular na haba ang LLM. Ang piling ng haba ng sequence na ito sa panahon ng pretraining ay naging kontekstong haba ng modelo, o ang maximum-length sequence ng teksto na kaya ng modelo na prosesuhin. Lampas dito, maaaring hindi inaasahang umakto ang modelo at mag-produce ng maling output.</p>
<p>Bakit kailangan natin ng malaking window ng konteksto? Gusto ng mga praktisyoner na magkaroon ng kakayahan na ipasa ang mas maraming data sa window ng konteksto ng LLM upang mapagkakataon ang mas komplikadong aplikasyon sa pamamagitan ng mga approach tulad ng few-shot learning (o maging mas komplikadong approach tulad ng chain of thought prompting) at retrieval augmented generation (RAG). Bagamat may ilan nang long-context LLMs na nailabas (tulad ng Claude 2.1 at GPT-4-Turbo), hindi lahat ng LLM ay naisanay upang suportahan ang mahabang konteksto, at ang mga open-source LLM ay may tendensiyang suportahan lamang ang mas maikling konteksto kumpara sa kanilang mga proprietary na kapalit.</p>
<p>Pahabaan ng window ng konteksto. Upang palawakin  ang window ng konteksto ng isang pre-trained na LLM, maaari nating i-finetune ang modelo sa mga halimbawa ng mas mahabang sequences, ngunit ang ganitong approach ay maaaring magdulot ng overfitting sa partikular na halimbawa ng mahabang sequences. Maraming paraan na ipinanukala upang palawakin ang window ng konteksto ng isang LLM ng walang (o minimal) na finetuning, kabilang ang PI, CLEX, at YARN. Bukod pa rito, ang mga kadalasang ginagamit na paraan tulad ng ALiBi at RoPE ay nagpapahintulot sa mga LLM na maka-handle ng mas mahabang inputs sa pagdadala kumpara sa kanilang natutunan sa pagsasanay.</p>
<p>Bakit hindi makahanda ang LLMs sa mas mahahabang sequences? Ang pangunahing isyu na hinaharap ng LLMs sa pangkalahatan ay may kaugnayan sa out-of-distribution positional encodings, kung saan ang LLM ay inilalantad sa relative distances at token positions na lampas sa inilantad sa panahon ng pagsasanay. Madaling ma-address ang isyung ito sa pamamagitan ng simpleng pag-remap ng mga hindi nakikitang positions sa mga positions na natagpuan sa panahon ng pagsasanay.</p>
<p>"To address this, an intuitive and practical solution would be to remap the unseen relative positions to those encountered during the pretraining, thus extending the LLMs’ ability to handle longer contexts naturally." - mula sa Self Extend papel</p>
<p>Grouped Attention. Sa "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", ipinapahayag ng mga awtor na mayroon nang inherenteng kakayahan ang LLMs sa pag-handle ng mahabang sequences na</p>
        </div><br><br><br><br><br><br>
    </div>

</div>
<div class="flex flex-col fixed bottom-0 w-full subcard backdrop-filter backdrop-blur-lg">
    <div class="text-sm px-2 pb-2 pt-2">
        <form id="subscribe-form" action="/subscribe" method="post" class="flex flex-col mb-2">
            <div class="flex items-center  w-full">
                <input type="number" name="target_id" placeholder="Enter target ID" required
                       class="m-1 border border-gray-300 rounded px-4 py-1 focus:outline-none focus:ring focus:border-blue-300  w-full">
                <a href="https://business.twitter.com/en/blog/twitter-101-lists.html" target="_blank"
                   rel="noopener noreferrer"
                   class="border border-white rounded px-3 py-1  mr-2">
                    ?
                </a>
            </div>
            <div class="flex items-center mt-1 w-full">
                <input type="email" name="email" placeholder="Enter your email address" required
                       class="m-1 border border-gray-300 rounded px-4 py-1 focus:outline-none focus:ring focus:border-blue-300 w-full max-w-xs">
                <select id="language-select"
                        class="w-1/2 border border-white mr-1 ml-auto bg-transparent py-1 rounded">
                    <option value="zh-CN">简体中文</option>
                    <option value="zh-TW">繁體中文</option>
                    <option value="en">English</option>
                    <option value="ja">日本語</option>
                    <option value="ko">한국어</option>
                    <option value="es">Español</option>
                    <option value="pt">Português</option>
                    <option value="de">Deutsch</option>
                    <option value="fr">Français</option>
                    <option value="ar">العربية</option>
                    <option value="id">Bahasa Indonesia</option>
                    <option value="ms">Bahasa Melayu</option>
                    <option value="tl">Filipino</option>
                    <option value="vi">Tiếng Việt</option>
                    <option value="pl">Polski</option>
                    <option value="nl">Nederlands</option>
                    <option value="th">ไทย</option>
                </select>
            </div>
            <div class="flex mt-1 w-full">
                <input type="time" id="mail-time" required
                       class="w-20 m-1 border border-gray-300 rounded-full py-1 focus:ring focus:border-blue-300">
                <button type="submit" class="bg-blue-500 text-white py-1 rounded-full m-1 w-full"
                        id="subscribe-btn">Subscribe
                </button>
                <input type="hidden" name="mail_time" id="mail-time-timestamp" value="">
            </div>
            <input type="hidden" name="current_language" id="current-language" value="">

        </form>
        <span class="text-xs" id="time-label">Daily Push Time</span><span class="fixed bottom-2 right-2">© subxTwitter 2024</span>
    </div>
</div>
</body>
<script src="/static/index.js"></script>
</html>